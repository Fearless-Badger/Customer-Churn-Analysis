{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will be working with data assosicated with bank customers. I will clean and analyze the data, in order to understand what our dataset looks like. After exploring the dataset, I will decide which model we should build. After deciding on which model to build, I will build and optimize a prediction model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The powershell in the assosiated GitHub repository allows for this notebook to download the datasetfor this project, provided you complete the \".env\" file with your kaggle API key, and file path.\n",
    "\n",
    "This script was written, so that this notebook may be run, tested, and modified either in the kaggle environment, or on a configured windows machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "import math\n",
    "import dotenv\n",
    "import kagglehub\n",
    "import os\n",
    "import subprocess\n",
    "import ipywidgets\n",
    "import random\n",
    "\n",
    "import sklearn_pandas as sk\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras as ks\n",
    "import matplotlib as mlt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import confusion_matrix, recall_score, f1_score, accuracy_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import anderson, lognorm, probplot, shapiro\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "try:\n",
    "    dotenv.load_dotenv()\n",
    "except:\n",
    "    print(\"--Dotenv not loaded--\")\n",
    "\n",
    "\n",
    "# Check for Kaggle environment and set the file path\n",
    "if os.path.exists(\"/kaggle/input/churn-modelling/Churn_Modelling.csv\"):\n",
    "    # Kaggle\n",
    "    file_path = \"/kaggle/input/churn-modelling/Churn_Modelling.csv\"\n",
    "else:\n",
    "    # Local\n",
    "    file_path = (str((os.getenv(\"LOCAL_FILE_LOCATION\"))))\n",
    "\n",
    "# Load Dataset\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Dataset Loaded Successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at : file_path\")\n",
    "    try:\n",
    "        print(\"Attempting to run download_data_ps1\")\n",
    "        path = os.getenv(\"SCRIPT_PATH\")\n",
    "        subprocess.run([\"powershell\", \"-ExecutionPolicy\", \"Bypass\", \"-File\", path],\n",
    "                       check = True, capture_output =  True, text = True)\n",
    "        print(\"Powershell Download Script Run Successfully. Now attempting to reload dataset...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        if df is not None and not df.empty:\n",
    "            print(\"Dataset Loaded Successfully\")\n",
    "        else:\n",
    "            print(\"Data not loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running powershell script: {e}\")\n",
    "        df = None\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "if df is not None:\n",
    "    display(df.head())\n",
    "\n",
    "# Set random seed for reproducability\n",
    "RANDOM = 123\n",
    "tf.keras.utils.set_random_seed(RANDOM)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "random.seed(RANDOM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is the last variable in this dataset. It is called \"Exited\". It is binary, where a 1 represents a customer closing thier account, and a 0 represents a retained customer.\n",
    "\n",
    "Let's preview the data in order to understand what we have to work with.\n",
    "\n",
    "First, I will drop the insignificant variables, which are the \"RowNumber\", \"CustomerId\", and \"Surname\" variables. They are arbitrary, and not useful for our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 3:-1]\n",
    "Y = df.iloc[:,-1:]\n",
    "\n",
    "display(X.head())\n",
    "display(Y.head())\n",
    "\n",
    "display(f\"{X.shape=}\")\n",
    "display(f\"{Y.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have 10,000 observations for both the predictor and target variables.\n",
    "\n",
    "Now, we will check the dataset for any Null values and duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = X.copy()\n",
    "new_df['Exited'] = Y\n",
    "\n",
    "print(df.isna().sum(), '\\n')\n",
    "print(f\"Duplicate Count   \", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\?'\n",
    "null_df = df[\"Surname\"].astype(str).str.contains(pattern)\n",
    "mark_count = 0\n",
    "for val in null_df: \n",
    "    if val : mark_count += 1\n",
    "display(f\"The number of question marks appearing in the surname column is : {mark_count}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"Surname\" column, there are rare instances of \"?\" appearing. This indicates that there is missing or incomplete names. This is not concerning, because the \"Surname\" variable will be discarded when we build our model. \n",
    "\n",
    "Since there are no concerning NA values or duplicates, we can proceed with encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will try to get a basic idea of what this dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.set_theme(rc={'axes.facecolor':'white', 'figure.facecolor':'lightgray'})\n",
    "colors = ['#432371',\"#FAAE7B\", '#D4314C', '#5E0303', '#504AEC']\n",
    "\n",
    "chart = sb.countplot(x = df[\"Exited\"], hue = df[\"Exited\"], palette=colors[0:2], legend=False, data = df);\n",
    "chart.set_title(\"Overall Customer Churn Outcome\")\n",
    "for container in chart.containers:\n",
    "    chart.bar_label(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of 7,963 customers were retained, and 2,037 customers churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sb.countplot(x = df[\"Exited\"], hue = df[\"HasCrCard\"], palette=colors[2:4]);\n",
    "chart.set_title(\"Churn for Credit Card Holders and Non-Card Holders\")\n",
    "for container in chart.containers:\n",
    "    chart.bar_label(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the people have credit cards, whether they churn or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sb.histplot(x = df['Age'], hue=df[\"Geography\"], kde = True, bins = 30, palette = colors[0:3]);\n",
    "chart.set_title(\"Histogram Plot of Age by Geographical Location\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Age   : \", round(df[\"Age\"].mean(),   1))\n",
    "print(f\"Median Age : \", round(df[\"Age\"].median(), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over half of the observations are french, and the average age is 38.9. The median age is 37.0. The mean vs median, combined with visually inspecting the plot above indicates that the 'Age' variable has a skew to the right.\n",
    "\n",
    "The sharp cutoff around 20 shows that the dataset does not include younger individuals, which could be explained by the bank having a minimum age requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_age = df[\"Age\"].max()\n",
    "min_age = df[\"Age\"].min()\n",
    "\n",
    "print(f\"Maximum Age  : {max_age}\")\n",
    "print(f\"Minimum Age  : {min_age}\")\n",
    "print(f\"Range of Age : {max_age-min_age}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right skew in Age is explained by the bank not allowing customers younger than 18 to bank with them. The skew occurs because the customers in the upper age range are fewer in number, yet still exist, as opposed to the lower age range (0-17) where these people are not allowed to be customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sb.boxplot(x = df[\"CreditScore\"], hue = df[\"Gender\"], palette=(colors[2], colors[4]))\n",
    "chart.legend(loc=\"upper left\")\n",
    "chart.set_title(\"Boxplot of Credit Score For Men and Women\")\n",
    "plt.show()\n",
    "\n",
    "mean_cr_score = df[\"CreditScore\"].mean()\n",
    "print(f\"Mean Credit Score : {round(mean_cr_score, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean credit score for both men and women is nearly the same. Some outliers stretch out to the left and below 400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sb.countplot(x = df[\"Gender\"], hue = df[\"NumOfProducts\"], palette=colors[0:4])\n",
    "chart.set_title(\"Gender Frequency By Number of Products\")\n",
    "legend = chart.legend(loc = 'upper right')\n",
    "legend.set_title(None)\n",
    "for container in chart.containers:\n",
    "    chart.bar_label(container)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of men and women in this dataset is almost even, with more men than women. Very few customers, regardless of gender, have more than 2 products. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First, we will examine the distributions of several variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.hist(bins = 30, figsize=(22, 10), color = '#D4314C');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this dataset is so large, it can be hard to tell exactly whether or not a distribution is normal. In order to proceed with my analysis, I will check for normality using the Anderson test.\n",
    "\n",
    "I picked the Anderson test for the creditscore variable since more weight is given to the tails of the distribution. This is Useful in this situation because of the sharp uptick at the right-tail of 'CreditScore'. \n",
    "\n",
    "Additionally, the Anderson test is suitable for large sets of observations. The Shapiro-Wilk test for normality would probably determine the 'CreditScore' variable to be normally distributed, since it is an unsuitable test for large inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    # Suppress warning. \n",
    "    # The warning states the shapiro wilk test is inaccurate for N > 5000.\n",
    "    # Current N is 10000\n",
    "    stat, p = shapiro(X['CreditScore'])\n",
    "    print(f\"Shapiro-Wilk Test: Stat = {round(stat, 3)}, p-val = {p}\\n\")\n",
    "\n",
    "\n",
    "result = anderson(X['CreditScore'])\n",
    "print(f\"Anderson Test: test-stat = {round(result.statistic, 3)}, Critical Values = {result.critical_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Anderson test statistic is 5.458. The critical values [0.576 0.656 0.787 0.918 1.092] correspond to significance levels [15%, 10%, 5%, 2.5% 1%]. Since the test statistic is greater than all critical values, we reject the null hypothesis. \n",
    "\n",
    "Compare this to the Shapiro-Wilk test. Since its p-value is less than 0.05, we would reject the null hypothesis and determine that the data is normally distributed. However, since the Shapiro test is not suitable for this dataset, we will disregard it.\n",
    "\n",
    "Therefore, the Anderson test determined that 'CreditScore' is not normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probplot(X['CreditScore'], dist = \"norm\", plot = plt)\n",
    "plt.title(\"Q-Q Plot of CreditScore\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probplot(X['EstimatedSalary'], dist = \"norm\", plot = plt)\n",
    "plt.title(\"Q-Q Plot of EstimatedSalary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the \"CreditScore\" Q-Q Plot, it is obviously not normally distributed. Also, I threw in the \"EstimatedSalary\" Q-Q plot. It follows a more uniform like distribution.\n",
    "\n",
    "We can visually inspect the previously produced histograms to determine that the \"Balance\" variable will behave simalarly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output variable is binary, we could use multiple logistic regression. Logistic regression does not require a normal distribution. However, it would require manual feature engineering to capture non-linear relationships, whereas a neural network is capable of automatically learning these patterns. This greatly cuts down on the amount of pre-processing work, since we only need to encode the categorical variables and scale the numeric features.\n",
    "\n",
    "The non-linear nature of this data, non-normal distribution, as well as the mix of categorical and quantitative data suggests that a neural network is a good model to build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon manual observation, the dataset is sufficiently shuffled. The observations are not sorted based on geography, balance, credit score, or any other variable. Therefore, we don't necessarily need to concern our selves with shuffling the dataset before building the model.\n",
    "\n",
    "Since we have two categorical variables, we must encode them before proceeding with the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjust_length = 17 # offset for output formatting\n",
    "\n",
    "for col in X:\n",
    "    print(f\"{col.ljust(adjust_length)} : {X[col].dtypes}\")\n",
    "print(f\"{(\"Exited\").ljust(adjust_length)} : {Y['Exited'].dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Gender\" variable will be encoded. A 0 represents the \"Female\" gender, a 1 represents the \"Male\" Gender.\n",
    "\n",
    "Similarly, the \"Geography\" variable will be encoded. Each geographical location will recieve its own binary column, with a 1 occurring in the column where the observation is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "X['Gender'] = Encoder.fit_transform(X.iloc[:, 2]).astype(int)\n",
    "\n",
    "display((X.loc[:, 'CreditScore':'Gender']).iloc[0 : 5])\n",
    "\n",
    "\n",
    "print(f\"{str(X['Gender'].name)} : {X['Gender'].dtypes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the \"Gender\" variable is represented as an integer.\n",
    "\n",
    "Next, we must encode the \"Geography\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, columns=['Geography'], drop_first=True)\n",
    "\n",
    "# Cast geography variables to integers\n",
    "for col in X.columns:\n",
    "    if 'Geography_' in col :\n",
    "        X[col] = X[col].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of the variables are represented numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df = X.copy()\n",
    "fin_df['Exited'] = Y\n",
    "\n",
    "display(fin_df.iloc[0 : 5])\n",
    "for col in fin_df:\n",
    "    print(f\"{col.ljust(adjust_length)} : {fin_df[col].dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform correlation analysis before scaling, but after encoding the data. This is since scaling affects the variance but not the relationship between the features. We analyze the correlation after encoding, since correlation analysis requires numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = fin_df.corr()\n",
    "plt.figure(figsize=(10,10))\n",
    "sb.heatmap(correlation_matrix, annot=True, cmap = 'coolwarm', fmt = \".2f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The row we will focus on the most is the \"Exited\" row. The strongest correlation is with \"Age\", which shows that age has a weak positive relation with \"Exited\". Other than that, there are only weakly correlations between the variables and the output.\n",
    "\n",
    "Other notable correlations show that german customers are moderately positively correlated with balance. Additionally, we can say there is a weak negative correlation with the number of products a customer has and their balance. \n",
    "\n",
    "This could indicate that as their balance goes down, the number of products may increase. Let's examine this relationship further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sb.histplot(x = fin_df[\"Balance\"], \n",
    "            hue = fin_df[\"NumOfProducts\"], \n",
    "            bins = 30, \n",
    "            kde = True, \n",
    "            palette = colors[0:4])\n",
    "chart.set_title(\"Balance and Number of Products\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the majority of observations have a balance of zero, this relationship loses a lot of credibility. The rest of the balance values seem normally distributed, with their number of products staying relatively low. It is notable that almost nobody has 4 products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the categorical variables have been encoded, we will scale the features. This step esures that the range of features is standardized.\n",
    "\n",
    "It is important that only the quantitative variables are scaled. Categorical variables are now represented as integers, and the scaler would scale them if we let it. This would destroy the categorical variables. \n",
    "\n",
    "To proceed, I will split the data into \"quantitative_cols\" and \"categorical_cols\". Only the quantitative data will be scaled, then the categorical data will be concatenated with the scaled quantitative data.\n",
    "\n",
    "I will detail this step with additional comments, since I have made this mistake in the past, and I see it in other notebooks often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data\n",
    "quantitative_cols = [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"EstimatedSalary\"]\n",
    "categorical_cols  = [\"Gender\", \"HasCrCard\", \"IsActiveMember\", \"Geography_Germany\", \"Geography_Spain\"]\n",
    "\n",
    "# Create train / test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=RANDOM)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale quantitative data\n",
    "X_train_scaled_quan = pd.DataFrame(scaler.fit_transform(X_train[quantitative_cols]), columns = quantitative_cols, index=X_train.index)\n",
    "\n",
    "# Do not fit test data, only scale\n",
    "X_test_scaled_quan  = pd.DataFrame(scaler.transform(X_test[quantitative_cols]), columns = quantitative_cols, index = X_test.index)\n",
    "\n",
    "# Concatenate scaled quantitative data with categorical data\n",
    "X_train_scaled = pd.concat([X_train_scaled_quan, X_train[categorical_cols]], axis=1)\n",
    "X_test_scaled  = pd.concat([X_test_scaled_quan, X_test[categorical_cols]], axis = 1)\n",
    "\n",
    "\n",
    "# Display scaled training data\n",
    "display(X_train_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the quantitative data has been scaled, and the binary representations of the categorical data has been preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will construct our tensorflow neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential() # initialize model\n",
    "\n",
    "model.add(layers.InputLayer(shape=(X_train_scaled.shape[1],))) # define shape\n",
    "\n",
    "model.add(layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01))) # Input layer\n",
    "model.add(layers.BatchNormalization())          # Normalize activations to stabilize and speed up training\n",
    "model.add(layers.Dropout(0.4))                  # Dropout layer to prevent overfitting\n",
    "model.add(layers.Dense(64, activation='relu')) \n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Save for later\n",
    "model2 = keras.models.clone_model(model)\n",
    "model3 = keras.models.clone_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will compile and train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will construct three models. The first is the most \"accurate\" and \"precise\" model. I believe the first model is the weakest, which I will explain.\n",
    "\n",
    "The second model weights the churned customers, so the neural network is punished more for miscassifying churned customers as \"safe\". \n",
    "\n",
    "The last model is recall optimized, yet it has a high rate of false positives and lower overall accuracy.\n",
    "\n",
    "Before we proceed, I will define some important terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RECALL is the Amount of true positive occurrances divided by the sum of true positives and false negatives. The recall statistic answers the question, \"Of all of the actual churned customers, how many did the model correctly predict as churned?\" The higher the recall, the more churned customers that the model correctly classified. A lower recall rate indicates that the model struggles to predict which customers will churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PRECISION is the amount of True positives divided by the sum of true positive and false positive occurrances. The precision statistic answers the quesion, \"How many of the predicted to churn customers actually churned?\" There is a tradeoff we need to consider between Recall and Precision. High precision often results in low recall, where we would catch fewer churned customers and avoid incorrectly classifying non-churned customers as churned. High recall would catch most churning customers, but may generate more false positives. In our case, we want to prioritize recall. This is because customer retention is much more important than preventing the incorrect classification of safe customers as churning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The F1-SCORE combines recall and precision. It has a more complex formula. It provides you with a balanced measure of a models performance. A low F1 score indicates a poor balance of Recall and Precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ACCURACY measures the proportion of correct predictions out of the total predictions made by the model. It answers the quesiont, \"How often is the model correct?\" Accuracy doesn't always tell the full story, in fact accuracy alone is a poor assessment of our model. In our case, non-churned customers dominate the dataset. Accuracy may be high even though the model may fail to predict the minority class accurately at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze training data, and construct early stopping object.\n",
    "\n",
    "\n",
    "# Stop when loss starts to plateau\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 15,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "# Squeeze and re-index the training output data, to avoid a ValueError\n",
    "Y_train = Y_train.squeeze()\n",
    "Y_train = Y_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Optimal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the cell below. Notice the class weights at the top of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes are evenly weighted (default behavior)\n",
    "class_weights = {0: 1.0, 1: 1.0}\n",
    "\n",
    "# Compile model\n",
    "tune2 = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model2.compile(optimizer=tune2,\n",
    "               loss = 'binary_crossentropy',\n",
    "               metrics = ['accuracy'])\n",
    "\n",
    "# Train sub optimal model\n",
    "history2 = model2.fit(X_train_scaled, Y_train,    \n",
    "                    class_weight=class_weights, \n",
    "                    epochs = 75,                \n",
    "                    batch_size = 32,            \n",
    "                    validation_split = 0.2,     \n",
    "                    callbacks = [early_stop],   \n",
    "                    verbose = 0)     \n",
    "\n",
    "# Cast predictions to integers\n",
    "raw_pred_bad_model = model2.predict(X_test_scaled)\n",
    "y_pred2 = (raw_pred_bad_model > 0.5).astype(\"int32\")\n",
    "\n",
    "# Create confusion matrix\n",
    "con_matrix2 = confusion_matrix(Y_test, y_pred2)\n",
    "\n",
    "# Display Confustion Matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sb.heatmap(con_matrix2, annot=True, fmt='d', cmap='Blues', cbar = False)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Poorly Optimized Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "def analyze_model(test_data, pred_data, name):\n",
    "    print(f\"{name} recall    = {recall_score(test_data, pred_data):.2f}\")\n",
    "    print(f\"{name} precision = {precision_score(test_data, pred_data):.2f}\")\n",
    "    print(f\"\\n\")\n",
    "    print(f\"{name} f1-Score  = {f1_score(test_data, pred_data):.2f}\")\n",
    "    print(f\"{name} accuracy  = {accuracy_score(test_data, pred_data):.2f}\")\n",
    "\n",
    "analyze_model(Y_test, y_pred2, \"Poorly optimized model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this un-weighted model, it is very good at predicting retained customers, but not useful for providing insights on which customers will actually leave the bank.\n",
    "\n",
    "This is the most \"precise\" and \"accurate\" model, but it is the poorest performing model. This is because the recall and the f1-score are the most important metrics for analyzing this dataset.\n",
    "\n",
    "The recall on this model is 0.38, which means that the model is very bad at guessing whether or not a churned customer will be retained or not. Each churned customer is worth a lot of money to the bank, so we want to minimize the recall as much as possible. Now I will make another model, where the churned customers are weighted 3 times more than retained customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is the original model we built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight the churned customers, so that the model will consider them more\n",
    "class_weights = {0: 1.0, 1: 3.5}\n",
    "\n",
    "tune = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=tune,\n",
    "              loss = \"binary_crossentropy\",\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Training\n",
    "history = model.fit(X_train_scaled, Y_train,    # Input data\n",
    "                    class_weight=class_weights, # Weight the churned customers\n",
    "                    epochs = 75,                # Max number of epochs\n",
    "                    batch_size = 32,            # Batch size\n",
    "                    validation_split = 0.2,     # Use 20% of the trianing data for validation\n",
    "                    callbacks = [early_stop],   # Use early stopping, to prevent overfitting\n",
    "                    verbose = 0)                # Suppress output\n",
    "\n",
    "# Cast predictions to integers\n",
    "raw_pred_balanced_model = model.predict(X_test_scaled)\n",
    "y_pred = (raw_pred_balanced_model > 0.5).astype(\"int32\")\n",
    "\n",
    "# Create confusion matrix\n",
    "con_matrix = confusion_matrix(Y_test, y_pred)\n",
    "\n",
    "# Display Confustion Matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sb.heatmap(con_matrix, annot=True, fmt='d', cmap='Blues', cbar = False)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Optimized Model Confusion Matrix\")\n",
    "\n",
    "# Save image for later\n",
    "if os.path.exists(\"/kaggle/\"):\n",
    "    save_path = \"/kaggle/working/\"\n",
    "else:\n",
    "    save_path = os.path.join(os.getcwd(), \"plots\")\n",
    "os.makedirs(save_path, exist_ok = True)\n",
    "plt.savefig(os.path.join(save_path, \"good_confusion_matrix.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Display statistics\n",
    "analyze_model(Y_test, y_pred, \"Optimized model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 1300 customers were retained, and the model classified them as such. 262 customers were mistakenly predicted to churn. 110 customers left the bank without the model predicting it. 304 customers were predicted to churn, and the prediction was accurate. \n",
    "\n",
    "Using \"class_weights = {0: 1.0, 1: 3.5}\", means that the neural network will penalize itself 3.5 times harder when it misclassifies a churned customer. The reason why we need to do this is becaues the amount of churned customers is low compared to the amount of retained customers. We adjust the class weight, to tell the model that even though the churned customers are low in frequency, they are important.\n",
    "\n",
    "Notice how this model's accuracy and especially precision are lower than the previous models. This tradeoff is acceptable, because we significantly increase our recall. This means that our optimized model is better able to predict which customers will churn better, instead of focusing so much on predicting which customers will be retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall Optimized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The recall rate could be further improved by increasing the class weight for churned customers. Even further, we could decrease the probability threshold for churned customers to 0.4, down from 0.5. This works, because the model outputs a series of probabilities. These probabilities represent the percentage chance that each specific customer churns. Previously, we used the 0.5 threshold. Decreasing it to 0.4 would classify a customer with a 43% chance of churning, as a churned customer.\n",
    "\n",
    "Increasing the recall comes at the cost of increasing the false positive ocurrance rate. Decreasing the threshold also increases the false positive rate.\n",
    "\n",
    "This is the line where the threshold is decreased :\n",
    "\n",
    "`y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")`\n",
    "\n",
    "In this model, I will increase the weight for churned customers to 4.0, and decrease the prediction threshold for churned customers to 0.4, down from 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churned customers class weight increased\n",
    "class_weights3 = {0: 1.0, 1: 4.0}# 3.0 -> 4.0\n",
    "\n",
    "# Recompile cloned model\n",
    "tune3 = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model3.compile(optimizer=tune3,\n",
    "               loss = 'binary_crossentropy',\n",
    "               metrics = ['accuracy'])\n",
    "\n",
    "# Train model 3\n",
    "history3 = model3.fit(X_train_scaled, Y_train,    \n",
    "                    class_weight=class_weights3, \n",
    "                    epochs = 75,                \n",
    "                    batch_size = 32,            \n",
    "                    validation_split = 0.2,     \n",
    "                    callbacks = [early_stop],   \n",
    "                    verbose = 0)     \n",
    "\n",
    "\n",
    "# Super recall optimized model\n",
    "raw_pred_recall_model = model3.predict(X_test_scaled)\n",
    "y_pred3 = (raw_pred_recall_model > 0.4).astype(\"int32\")# 0.5 -> 0.4\n",
    "con_matrix3 = confusion_matrix(Y_test, y_pred3)\n",
    "\n",
    "# Display Confustion Matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sb.heatmap(con_matrix3, annot=True, fmt='d', cmap='Blues', cbar = False)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Recall Optimized Matrix\")\n",
    "plt.show()\n",
    "\n",
    "analyze_model(Y_test, y_pred3, \"Recall optimized model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this threshold, the recall is significantly improved, and the false positive rate is also increased. \n",
    "\n",
    "Notice how the overall accuracy decreases. This would be an acceptable trade off if we needed to minimize false negatives as much as possible.\n",
    "\n",
    "We will finish off with some final exploratory data analysis to help us decide which model is best.\n",
    "\n",
    "Let's take a look at the bank account balance of our customers, and their churn outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Median bank account balance : {fin_df[\"Balance\"].median():.2f}\")\n",
    "print(f\"Mean bank account balance   : {fin_df[\"Balance\"].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average balance of the bank customers is 76,485. Remember, A lot of the bank's customers have a balance of zero. The 0 balance customers bring the average down significantly. Let's revisit the balance distribution, and see where the churned customers are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sb.histplot(x = fin_df['Balance'], hue=fin_df['Exited'], kde =True, element = 'step', palette=colors[2:4]);\n",
    "chart.set_title(\"Churned Customers And Balance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 500 out of the 2037 churned customers have no balance. This indicates that they are some other type of customer, not an account holder. These customers could bring the bank revenue in other ways. Determining their value is not as straightforward as examing their balance. As this information is not available to us, we will assume that each customer is equally valuable in the eyes of the bank, for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing which model to use depends on the bank's churn prevention strategty. If their strategy is costly, it would be beneficial to pick the most balanced model. This model has a good ability to predict churned customers, and keeps the amount of false positives fairly low. This would be a good pick for a bank with a costly churn prevention strategy, because the bank would not waste too much money on customers that would have stayed with the bank anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(os.path.join(save_path, \"good_confusion_matrix.png\"), width = 600))\n",
    "analyze_model(Y_test, y_pred, \"Optimized model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I belive this model is the best fit for this dataset. It has 81% overall accuracy, and it has a 73% recall rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on this project has taught me how important it is to understand your dataset. Model accuracy alone does not give you the whole picture, it is important to step back and assess what your model must do, and focus on delivering a solution that fits your situation. \n",
    "\n",
    "If a bank had exclusively high balance customers, then it would be in that banks best interest to use the recall optimized model. For a more general bank with typical customers, the balanced model provides the best \"one-size-fits-all\" solution. This model balances the false positive rate with recall, allowing the bank to maximize revenue.\n",
    "\n",
    "Further model development could potentially involve dynamically decreasing the prediction threshold for customers with a very high balance. This would be a good idea since customers with a high balance are very valuable to the bank. \n",
    "\n",
    "I'm sure this is a strategy real banks use, as high balance customers are very valuable and they could afford to lose some money chasing after false positives with high balances, as they are fewer in number compared to customers with less money."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I created an import script which detects if the \"Churn_Modelling.csv\" is downloaded or not. If we need to download the dataeset, it retrieves it, whether you are on a windows machine or in the kaggle development environment. \n",
    "\n",
    "Then I cleaned the data, and performed exploratory analysis. This analysis was key, as it formed the foundation of my decision to build a neural network. I built 3 different models and compared them all to each other. After trianing the models, I deteremined that the balanced model was the best pick. I picked the balanced model because of its ability to correctly classify the churned customers, while keeping the false positive occurrances relatively low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dear Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for taking the time to read through my notebook.\n",
    "\n",
    "If you are the collaborative type and would like to expand on my work, feel free to clone my [github repository](https://github.com/Fearless-Badger/Customer-Churn-Analysis) and submit a pull request.\n",
    "\n",
    "If you learned something, please consider leaving a vote and sharing what you learned!\n",
    "\n",
    "If you spot a mistake or see an improvement that could be made, please leave a comment detailing the issue and I will work to fix it. Continuous learning is important to me, and I am always looking to optimize my work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
