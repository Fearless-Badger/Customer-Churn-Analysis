{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Dataset and Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The powershell in the assosiated GitHub repository allows for this notebook to download the datasetfor this project, provided you complete the \".env\" file with your kaggle API key, and file path.\n",
    "\n",
    "This script was written, so that this notebook may be run, tested, and modified either in the kaggle environment, or on a configured windows machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "import math\n",
    "import dotenv\n",
    "import kagglehub\n",
    "import os\n",
    "import subprocess\n",
    "import ipywidgets\n",
    "\n",
    "\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras as ks\n",
    "import matplotlib as mlt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "\n",
    "from scipy.stats import anderson, lognorm, probplot, shapiro\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "try:\n",
    "    dotenv.load_dotenv()\n",
    "except:\n",
    "    print(\"--Dotenv not loaded--\")\n",
    "\n",
    "\n",
    "# Check for Kaggle environment and set the file path\n",
    "if os.path.exists(\"/kaggle/input/churn-modelling/Churn_Modelling.csv\"):\n",
    "    # Kaggle\n",
    "    file_path = \"/kaggle/input/churn-modelling/Churn_Modelling.csv\"\n",
    "else:\n",
    "    # Local\n",
    "    file_path = (str((os.getenv(\"LOCAL_FILE_LOCATION\"))))\n",
    "\n",
    "# Load Dataset\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Dataset Loaded Successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at : file_path\")\n",
    "    try:\n",
    "        print(\"Attempting to run download_data_ps1\")\n",
    "        path = os.getenv(\"SCRIPT_PATH\")\n",
    "        subprocess.run([\"powershell\", \"-ExecutionPolicy\", \"Bypass\", \"-File\", path],\n",
    "                       check = True, capture_output =  True, text = True)\n",
    "        print(\"Powershell Download Script Run Successfully. Now attempting to reload dataset...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        if df is not None and not df.empty:\n",
    "            print(\"Dataset Loaded Successfully\")\n",
    "        else:\n",
    "            print(\"Data not loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running powershell script: {e}\")\n",
    "        df = None\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "if df is not None:\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is the last variable in this dataset. It is called \"Exited\". It is binary, where a 1 represents a customer closing thier account, and a 0 represents a retained customer.\n",
    "\n",
    "Let's preview the data in order to understand what we have to work with.\n",
    "\n",
    "First, I will drop the insignificant variables, which are the \"RowNumber\", \"CustomerId\", and \"Surname\" variables. They are arbitrary, and not useful for our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM = 379\n",
    "\n",
    "X = df.iloc[:, 3:-1]\n",
    "Y = df.iloc[:,-1:]\n",
    "\n",
    "display(X.head())\n",
    "display(Y.head())\n",
    "\n",
    "display(f\"{X.shape=}\")\n",
    "display(f\"{Y.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have 10,000 observations for both the predictor and target variables.\n",
    "\n",
    "Now, we will check the dataset for any Null values and duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = X.copy()\n",
    "new_df['Exited'] = Y\n",
    "\n",
    "print(df.isna().sum(), '\\n')\n",
    "print(f\"Duplicate Count   \", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\?'\n",
    "null_df = df[\"Surname\"].astype(str).str.contains(pattern)\n",
    "mark_count = 0\n",
    "for val in null_df: \n",
    "    if val : mark_count += 1\n",
    "display(f\"The number of question marks appearing in the surname column is : {mark_count}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"Surname\" column, there are rare instances of \"?\" appearing. This indicates that there is missing or incomplete names. This is not concerning, because the \"Surname\" variable will be discarded when we build our model. \n",
    "\n",
    "Since there are no concerning NA values or duplicates, we can proceed with encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will try to get a basic idea of what this dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.set_theme(rc={'axes.facecolor':'white', 'figure.facecolor':'lightgray'})\n",
    "colors = ['#432371',\"#FAAE7B\", '#D4314C', '#5E0303', '#504AEC']\n",
    "\n",
    "chart = sb.countplot(x = df[\"Exited\"], hue = df[\"Exited\"], palette=colors[0:2], legend=False, data = df);\n",
    "chart.set_title(\"Overall Customer Churn Outcome\")\n",
    "for container in chart.containers:\n",
    "    chart.bar_label(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of 7,963 customers were retained, and 2,037 customers churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sb.countplot(x = df[\"Exited\"], hue = df[\"HasCrCard\"], palette=colors[2:4]);\n",
    "chart.set_title(\"Churn for Credit Card Holders and Non-Card Holders\")\n",
    "for container in chart.containers:\n",
    "    chart.bar_label(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the people have credit cards, whether they churn or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sb.histplot(x = df['Age'], hue=df[\"Geography\"], kde = True, bins = 30, palette = colors[0:3]);\n",
    "chart.set_title(\"Histogram Plot of Age by Geographical Location\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Age   : \", round(df[\"Age\"].mean(),   1))\n",
    "print(f\"Median Age : \", round(df[\"Age\"].median(), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over half of the observations are french, and the average age is 38.9. The median age is 37.0. The mean vs median, combined with visually inspecting the plot above indicates that the 'Age' variable has a skew to the right.\n",
    "\n",
    "The sharp cutoff around 20 shows that the dataset does not include younger individuals, which could be explained by the bank having a minimum age requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_age = df[\"Age\"].max()\n",
    "min_age = df[\"Age\"].min()\n",
    "\n",
    "print(f\"Maximum Age  : {max_age}\")\n",
    "print(f\"Minimum Age  : {min_age}\")\n",
    "print(f\"Range of Age : {max_age-min_age}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right skew in Age is explained by the bank not allowing customers younger than 18 to bank with them. The skew occurs because the customers in the upper age range are fewer in number, yet still exist, as opposed to the lower age range (0-17) where these people are not allowed to be customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sb.boxplot(x = df[\"CreditScore\"], hue = df[\"Gender\"], palette=(colors[2], colors[4]))\n",
    "chart.legend(loc=\"upper left\")\n",
    "chart.set_title(\"Boxplot of Credit Score For Men and Women\")\n",
    "plt.show()\n",
    "\n",
    "mean_cr_score = df[\"CreditScore\"].mean()\n",
    "print(f\"Mean Credit Score : {round(mean_cr_score, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean credit score for both men and women is nearly the same. Some outliers stretch out to the left and below 400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sb.countplot(x = df[\"Gender\"], hue = df[\"NumOfProducts\"], palette=colors[0:4])\n",
    "chart.set_title(\"Gender Frequency By Number of Products\")\n",
    "legend = chart.legend(loc = 'upper right')\n",
    "legend.set_title(None)\n",
    "for container in chart.containers:\n",
    "    chart.bar_label(container)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of men and women in this dataset is almost even, with more men than women. Very few customers, regardless of gender, have more than 2 products. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First, we will examine the distributions of several variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.hist(bins = 30, figsize=(22, 10), color = '#D4314C');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this dataset is so large, it can be hard to tell exactly whether or not a distribution is normal. In order to proceed with my analysis, I will check for normality using the Anderson test.\n",
    "\n",
    "I picked the Anderson test for the creditscore variable since more weight is given to the tails of the distribution. This is Useful in this situation because of the sharp uptick at the right-tail of 'CreditScore'. \n",
    "\n",
    "Additionally, the Anderson test is suitable for large sets of observations. The Shapiro-Wilk test for normality would probably determine the 'CreditScore' variable to be normally distributed, since it is an unsuitable test for large inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    # Suppress warning. \n",
    "    # The warning states the shapiro wilk test is inaccurate for N > 5000.\n",
    "    # Current N is 10000\n",
    "    stat, p = shapiro(X['CreditScore'])\n",
    "    print(f\"Shapiro-Wilk Test: Stat = {round(stat, 3)}, p-val = {p}\\n\")\n",
    "\n",
    "\n",
    "result = anderson(X['CreditScore'])\n",
    "print(f\"Anderson Test: test-stat = {round(result.statistic, 3)}, Critical Values = {result.critical_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Anderson test statistic is 5.458. The critical values [0.576 0.656 0.787 0.918 1.092] correspond to significance levels [15%, 10%, 5%, 2.5% 1%]. Since the test statistic is greater than all critical values, we reject the null hypothesis. \n",
    "\n",
    "Compare this to the Shapiro-Wilk test. Since its p-value is less than 0.05, we would reject the null hypothesis and determine that the data is normally distributed. However, since the Shapiro test is not suitable for this dataset, we will disregard it.\n",
    "\n",
    "Therefore, the Anderson test determined that 'CreditScore' is not normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probplot(X['CreditScore'], dist = \"norm\", plot = plt)\n",
    "plt.title(\"Q-Q Plot of CreditScore\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probplot(X['EstimatedSalary'], dist = \"norm\", plot = plt)\n",
    "plt.title(\"Q-Q Plot of EstimatedSalary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the \"CreditScore\" Q-Q Plot, it is obviously not normally distributed. Also, I threw in the \"EstimatedSalary\" Q-Q plot. It follows a more uniform like distribution.\n",
    "\n",
    "We can visually inspect the previously produced histograms to determine that the \"Balance\" variable will behave simalarly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is not normally distributed, we need a non-parametric model. Since the output is binary, we could use a multiple logistic regression model. Logistic regression does not require a normal distribution. However, it would require manual feature engineering to capture non-linear relationships, whereas a neural network is capable of automatically learning these patterns. This greatly cuts down on the amount of pre-processing work, since we only need to encode the categorical variables and scale all the features.\n",
    "\n",
    "The non-linear nature of this data, non-normal distribution, as well as the mix of categorical and quantitative data suggests that a neural network is a good model to build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building The Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon manual observation, the dataset is sufficiently shuffled. The observations are not sorted based on geography, balance, credit score, or any other variable. Therefore, we don't necessarily need to concern our selves with shuffling the dataset before building the model.\n",
    "\n",
    "Since we have two categorical variables, we must encode them before proceeding with the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjust_length = 17 # offset for output formatting\n",
    "\n",
    "for col in X:\n",
    "    print(f\"{col.ljust(adjust_length)} : {X[col].dtypes}\")\n",
    "print(f\"{(\"Exited\").ljust(adjust_length)} : {Y['Exited'].dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Gender\" variable will be encoded. A 0 represents the \"Female\" gender, a 1 represents the \"Male\" Gender.\n",
    "\n",
    "Similarly, the \"Geography\" variable will be encoded. Each geographical location will recieve its own binary column, with a 1 occurring in the column where the observation is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "X['Gender'] = Encoder.fit_transform(X.iloc[:, 2]).astype(int)\n",
    "\n",
    "display((X.loc[:, 'CreditScore':'Gender']).iloc[0 : 5])\n",
    "\n",
    "\n",
    "print(f\"{str(X['Gender'].name)} : {X['Gender'].dtypes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the \"Gender\" variable is represented as an integer.\n",
    "\n",
    "Next, we must encode the \"Geography\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, columns=['Geography'], drop_first=True)\n",
    "\n",
    "# Cast geography variables to integers\n",
    "for col in X.columns:\n",
    "    if 'Geography_' in col :\n",
    "        X[col] = X[col].astype('int64')\n",
    "\n",
    "for col in X:\n",
    "    print(f\"{col.ljust(adjust_length)} : {X[col].dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of the variables are represented numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df = X.copy()\n",
    "fin_df['Exited'] = Y\n",
    "\n",
    "display(fin_df.iloc[0 : 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "to-do: \n",
    "\n",
    "       - Perform basic EDA BEFORE encoding / scaling\n",
    "       - Scale features\n",
    "       - Perform EDA AFTER encoding/scaling\n",
    "       - Build neural network\n",
    "       - Optimize NN\n",
    "       - Analyze NN accuracy\n",
    "       - Review + Document\n",
    "       - Publish\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
