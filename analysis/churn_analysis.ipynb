{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The powershell in the assosiated GitHub repository allows for this notebook to download the datasetfor this project, provided you complete the \".env\" file with your kaggle API key, and file path.\n",
    "\n",
    "This script was written, so that this notebook may be run, tested, and modified either in the kaggle environment, or on a configured windows machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "import math\n",
    "import dotenv\n",
    "import kagglehub\n",
    "import os\n",
    "import subprocess\n",
    "import ipywidgets\n",
    "import random\n",
    "\n",
    "import sklearn_pandas as sk\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras as ks\n",
    "import matplotlib as mlt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import confusion_matrix, recall_score, f1_score, accuracy_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import anderson, lognorm, probplot, shapiro\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "try:\n",
    "    dotenv.load_dotenv()\n",
    "except:\n",
    "    print(\"--Dotenv not loaded--\")\n",
    "\n",
    "\n",
    "# Check for Kaggle environment and set the file path\n",
    "if os.path.exists(\"/kaggle/input/churn-modelling/Churn_Modelling.csv\"):\n",
    "    # Kaggle\n",
    "    file_path = \"/kaggle/input/churn-modelling/Churn_Modelling.csv\"\n",
    "else:\n",
    "    # Local\n",
    "    file_path = (str((os.getenv(\"LOCAL_FILE_LOCATION\"))))\n",
    "\n",
    "# Load Dataset\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Dataset Loaded Successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at : file_path\")\n",
    "    try:\n",
    "        print(\"Attempting to run download_data_ps1\")\n",
    "        path = os.getenv(\"SCRIPT_PATH\")\n",
    "        subprocess.run([\"powershell\", \"-ExecutionPolicy\", \"Bypass\", \"-File\", path],\n",
    "                       check = True, capture_output =  True, text = True)\n",
    "        print(\"Powershell Download Script Run Successfully. Now attempting to reload dataset...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        if df is not None and not df.empty:\n",
    "            print(\"Dataset Loaded Successfully\")\n",
    "        else:\n",
    "            print(\"Data not loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running powershell script: {e}\")\n",
    "        df = None\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "if df is not None:\n",
    "    display(df.head())\n",
    "\n",
    "# Set random seed for reproducability\n",
    "RANDOM = 123\n",
    "tf.keras.utils.set_random_seed(RANDOM)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "random.seed(RANDOM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is the last variable in this dataset. It is called \"Exited\". It is binary, where a 1 represents a customer closing thier account, and a 0 represents a retained customer.\n",
    "\n",
    "Let's preview the data in order to understand what we have to work with.\n",
    "\n",
    "First, I will drop the insignificant variables, which are the \"RowNumber\", \"CustomerId\", and \"Surname\" variables. They are arbitrary, and not useful for our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 3:-1]\n",
    "Y = df.iloc[:,-1:]\n",
    "\n",
    "display(X.head())\n",
    "display(Y.head())\n",
    "\n",
    "display(f\"{X.shape=}\")\n",
    "display(f\"{Y.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have 10,000 observations for both the predictor and target variables.\n",
    "\n",
    "Now, we will check the dataset for any Null values and duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = X.copy()\n",
    "new_df['Exited'] = Y\n",
    "\n",
    "print(df.isna().sum(), '\\n')\n",
    "print(f\"Duplicate Count   \", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\?'\n",
    "null_df = df[\"Surname\"].astype(str).str.contains(pattern)\n",
    "mark_count = 0\n",
    "for val in null_df: \n",
    "    if val : mark_count += 1\n",
    "display(f\"The number of question marks appearing in the surname column is : {mark_count}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"Surname\" column, there are rare instances of \"?\" appearing. This indicates that there is missing or incomplete names. This is not concerning, because the \"Surname\" variable will be discarded when we build our model. \n",
    "\n",
    "Since there are no concerning NA values or duplicates, we can proceed with encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will try to get a basic idea of what this dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.set_theme(rc={'axes.facecolor':'white', 'figure.facecolor':'lightgray'})\n",
    "colors = ['#432371',\"#FAAE7B\", '#D4314C', '#5E0303', '#504AEC']\n",
    "\n",
    "chart = sb.countplot(x = df[\"Exited\"], hue = df[\"Exited\"], palette=colors[0:2], legend=False, data = df);\n",
    "chart.set_title(\"Overall Customer Churn Outcome\")\n",
    "for container in chart.containers:\n",
    "    chart.bar_label(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of 7,963 customers were retained, and 2,037 customers churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sb.countplot(x = df[\"Exited\"], hue = df[\"HasCrCard\"], palette=colors[2:4]);\n",
    "chart.set_title(\"Churn for Credit Card Holders and Non-Card Holders\")\n",
    "for container in chart.containers:\n",
    "    chart.bar_label(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the people have credit cards, whether they churn or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sb.histplot(x = df['Age'], hue=df[\"Geography\"], kde = True, bins = 30, palette = colors[0:3]);\n",
    "chart.set_title(\"Histogram Plot of Age by Geographical Location\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Age   : \", round(df[\"Age\"].mean(),   1))\n",
    "print(f\"Median Age : \", round(df[\"Age\"].median(), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over half of the observations are french, and the average age is 38.9. The median age is 37.0. The mean vs median, combined with visually inspecting the plot above indicates that the 'Age' variable has a skew to the right.\n",
    "\n",
    "The sharp cutoff around 20 shows that the dataset does not include younger individuals, which could be explained by the bank having a minimum age requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_age = df[\"Age\"].max()\n",
    "min_age = df[\"Age\"].min()\n",
    "\n",
    "print(f\"Maximum Age  : {max_age}\")\n",
    "print(f\"Minimum Age  : {min_age}\")\n",
    "print(f\"Range of Age : {max_age-min_age}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right skew in Age is explained by the bank not allowing customers younger than 18 to bank with them. The skew occurs because the customers in the upper age range are fewer in number, yet still exist, as opposed to the lower age range (0-17) where these people are not allowed to be customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sb.boxplot(x = df[\"CreditScore\"], hue = df[\"Gender\"], palette=(colors[2], colors[4]))\n",
    "chart.legend(loc=\"upper left\")\n",
    "chart.set_title(\"Boxplot of Credit Score For Men and Women\")\n",
    "plt.show()\n",
    "\n",
    "mean_cr_score = df[\"CreditScore\"].mean()\n",
    "print(f\"Mean Credit Score : {round(mean_cr_score, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean credit score for both men and women is nearly the same. Some outliers stretch out to the left and below 400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sb.countplot(x = df[\"Gender\"], hue = df[\"NumOfProducts\"], palette=colors[0:4])\n",
    "chart.set_title(\"Gender Frequency By Number of Products\")\n",
    "legend = chart.legend(loc = 'upper right')\n",
    "legend.set_title(None)\n",
    "for container in chart.containers:\n",
    "    chart.bar_label(container)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of men and women in this dataset is almost even, with more men than women. Very few customers, regardless of gender, have more than 2 products. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First, we will examine the distributions of several variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.hist(bins = 30, figsize=(22, 10), color = '#D4314C');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this dataset is so large, it can be hard to tell exactly whether or not a distribution is normal. In order to proceed with my analysis, I will check for normality using the Anderson test.\n",
    "\n",
    "I picked the Anderson test for the creditscore variable since more weight is given to the tails of the distribution. This is Useful in this situation because of the sharp uptick at the right-tail of 'CreditScore'. \n",
    "\n",
    "Additionally, the Anderson test is suitable for large sets of observations. The Shapiro-Wilk test for normality would probably determine the 'CreditScore' variable to be normally distributed, since it is an unsuitable test for large inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    # Suppress warning. \n",
    "    # The warning states the shapiro wilk test is inaccurate for N > 5000.\n",
    "    # Current N is 10000\n",
    "    stat, p = shapiro(X['CreditScore'])\n",
    "    print(f\"Shapiro-Wilk Test: Stat = {round(stat, 3)}, p-val = {p}\\n\")\n",
    "\n",
    "\n",
    "result = anderson(X['CreditScore'])\n",
    "print(f\"Anderson Test: test-stat = {round(result.statistic, 3)}, Critical Values = {result.critical_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Anderson test statistic is 5.458. The critical values [0.576 0.656 0.787 0.918 1.092] correspond to significance levels [15%, 10%, 5%, 2.5% 1%]. Since the test statistic is greater than all critical values, we reject the null hypothesis. \n",
    "\n",
    "Compare this to the Shapiro-Wilk test. Since its p-value is less than 0.05, we would reject the null hypothesis and determine that the data is normally distributed. However, since the Shapiro test is not suitable for this dataset, we will disregard it.\n",
    "\n",
    "Therefore, the Anderson test determined that 'CreditScore' is not normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probplot(X['CreditScore'], dist = \"norm\", plot = plt)\n",
    "plt.title(\"Q-Q Plot of CreditScore\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probplot(X['EstimatedSalary'], dist = \"norm\", plot = plt)\n",
    "plt.title(\"Q-Q Plot of EstimatedSalary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the \"CreditScore\" Q-Q Plot, it is obviously not normally distributed. Also, I threw in the \"EstimatedSalary\" Q-Q plot. It follows a more uniform like distribution.\n",
    "\n",
    "We can visually inspect the previously produced histograms to determine that the \"Balance\" variable will behave simalarly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output variable is binary, we could use multiple logistic regression. Logistic regression does not require a normal distribution. However, it would require manual feature engineering to capture non-linear relationships, whereas a neural network is capable of automatically learning these patterns. This greatly cuts down on the amount of pre-processing work, since we only need to encode the categorical variables and scale the numeric features.\n",
    "\n",
    "The non-linear nature of this data, non-normal distribution, as well as the mix of categorical and quantitative data suggests that a neural network is a good model to build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon manual observation, the dataset is sufficiently shuffled. The observations are not sorted based on geography, balance, credit score, or any other variable. Therefore, we don't necessarily need to concern our selves with shuffling the dataset before building the model.\n",
    "\n",
    "Since we have two categorical variables, we must encode them before proceeding with the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjust_length = 17 # offset for output formatting\n",
    "\n",
    "for col in X:\n",
    "    print(f\"{col.ljust(adjust_length)} : {X[col].dtypes}\")\n",
    "print(f\"{(\"Exited\").ljust(adjust_length)} : {Y['Exited'].dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Gender\" variable will be encoded. A 0 represents the \"Female\" gender, a 1 represents the \"Male\" Gender.\n",
    "\n",
    "Similarly, the \"Geography\" variable will be encoded. Each geographical location will recieve its own binary column, with a 1 occurring in the column where the observation is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "X['Gender'] = Encoder.fit_transform(X.iloc[:, 2]).astype(int)\n",
    "\n",
    "display((X.loc[:, 'CreditScore':'Gender']).iloc[0 : 5])\n",
    "\n",
    "\n",
    "print(f\"{str(X['Gender'].name)} : {X['Gender'].dtypes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the \"Gender\" variable is represented as an integer.\n",
    "\n",
    "Next, we must encode the \"Geography\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, columns=['Geography'], drop_first=True)\n",
    "\n",
    "# Cast geography variables to integers\n",
    "for col in X.columns:\n",
    "    if 'Geography_' in col :\n",
    "        X[col] = X[col].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of the variables are represented numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df = X.copy()\n",
    "fin_df['Exited'] = Y\n",
    "\n",
    "display(fin_df.iloc[0 : 5])\n",
    "for col in fin_df:\n",
    "    print(f\"{col.ljust(adjust_length)} : {fin_df[col].dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform correlation analysis before scaling, but after encoding the data. This is since scaling affects the variance but not the relationship between the features. We analyze the correlation after encoding, since correlation analysis requires numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = fin_df.corr()\n",
    "plt.figure(figsize=(10,10))\n",
    "sb.heatmap(correlation_matrix, annot=True, cmap = 'coolwarm', fmt = \".2f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The row we will focus on the most is the \"Exited\" row. The strongest correlation is with \"Age\", which shows that age has a weak positive relation with \"Exited\". Other than that, there are only weakly correlations between the variables and the output.\n",
    "\n",
    "Other notable correlations show that german customers are moderately positively correlated with balance. Additionally, we can say there is a weak negative correlation with the number of products a customer has and their balance. \n",
    "\n",
    "This could indicate that as their balance goes down, the number of products may increase. Let's examine this relationship further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sb.histplot(x = fin_df[\"Balance\"], \n",
    "            hue = fin_df[\"NumOfProducts\"], \n",
    "            bins = 30, \n",
    "            kde = True, \n",
    "            palette = colors[0:4])\n",
    "chart.set_title(\"Balance and Number of Products\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the majority of observations have a balance of zero, this relationship loses a lot of credibility. The rest of the balance values seem normally distributed, with their number of products staying relatively low. It is notable that almost nobody has 4 products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the categorical variables have been encoded, we will scale the features. This step esures that the range of features is standardized.\n",
    "\n",
    "It is important that only the quantitative variables are scaled. Categorical variables are now represented as integers, and the scaler would scale them if we let it. This would destroy the categorical variables. \n",
    "\n",
    "To proceed, I will split the data into \"quantitative_cols\" and \"categorical_cols\". Only the quantitative data will be scaled, then the categorical data will be concatenated with the scaled quantitative data.\n",
    "\n",
    "I will detail this step with additional comments, since I have made this mistake in the past, and I see it in other notebooks often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data\n",
    "quantitative_cols = [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"EstimatedSalary\"]\n",
    "categorical_cols  = [\"Gender\", \"HasCrCard\", \"IsActiveMember\", \"Geography_Germany\", \"Geography_Spain\"]\n",
    "\n",
    "# Create train / test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=RANDOM)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale quantitative data\n",
    "X_train_scaled_quan = pd.DataFrame(scaler.fit_transform(X_train[quantitative_cols]), columns = quantitative_cols, index=X_train.index)\n",
    "\n",
    "# Do not fit test data, only scale\n",
    "X_test_scaled_quan  = pd.DataFrame(scaler.transform(X_test[quantitative_cols]), columns = quantitative_cols, index = X_test.index)\n",
    "\n",
    "# Concatenate scaled quantitative data with categorical data\n",
    "X_train_scaled = pd.concat([X_train_scaled_quan, X_train[categorical_cols]], axis=1)\n",
    "X_test_scaled  = pd.concat([X_test_scaled_quan, X_test[categorical_cols]], axis = 1)\n",
    "\n",
    "\n",
    "# Display scaled training data\n",
    "display(X_train_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the quantitative data has been scaled, and the binary representations of the categorical data has been preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will construct our tensorflow neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential() # initialize model\n",
    "\n",
    "model.add(layers.InputLayer(shape=(X_train_scaled.shape[1],))) # define shape\n",
    "\n",
    "model.add(layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01))) # Input layer\n",
    "model.add(layers.BatchNormalization())          # Normalize activations to stabilize and speed up training\n",
    "model.add(layers.Dropout(0.4))                  # Dropout layer to prevent overfitting\n",
    "model.add(layers.Dense(64, activation='relu')) \n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Save for later\n",
    "model2 = keras.models.clone_model(model)\n",
    "model3 = keras.models.clone_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will compile and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate tuning\n",
    "tune = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compilation\n",
    "model.compile(optimizer=tune,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Stop when loss starts to plateau\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 15,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "# Weight the churned customers, so that the model will consider them more\n",
    "class_weights = {0: 1.0, 1: 3.5}\n",
    "\n",
    "# Squeeze and re-index the training output data, to avoid a ValueError\n",
    "Y_train = Y_train.squeeze()\n",
    "Y_train = Y_train.reset_index(drop=True)\n",
    "\n",
    "# Training\n",
    "history = model.fit(X_train_scaled, Y_train,    # Input data\n",
    "                    class_weight=class_weights, # Weight the churned customers\n",
    "                    epochs = 75,                # Max number of epochs\n",
    "                    batch_size = 32,            # Batch size\n",
    "                    validation_split = 0.2,     # Use 20% of the trianing data for validation\n",
    "                    callbacks = [early_stop],   # Use early stopping, to prevent overfitting\n",
    "                    verbose = 0)                # Suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the Neural Network has been trained, and it is ready to be analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will construct three models. The first is the most balanced and optimized model. The second the technically more accurate, but inferior to the first model because of its poor recall. The last model is recall optimized, yet it has a high rate of false positives.\n",
    "\n",
    "Before we proceed, I will define some terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is the Amount of true positive occurrances divided by the sum of true positives and false negatives. The recall statistic answers the question, \"Of all of the actual churned customers, how many did the model correctly predict as churned?\"\n",
    "\n",
    "The higher the recall, the more churned customers that the model correctly classified. A lower recall rate indicates that the model struggles to predict which customers will churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is the amount of True positives divided by the sum of true positive and false positive occurrances. The precision statistic answers the quesion, \"How many of the predicted to churn customers actually churned?\"\n",
    "\n",
    "There is a tradeoff we need to consider between Recall and Precision. \n",
    "\n",
    "High precision often results in low recall, where we would catch fewer churned customers and avoid incorrectly classifying non-churned customers as churned.\n",
    "\n",
    "High recall would catch most churning customers, but may generate more false positives.\n",
    "\n",
    "In our case, we want to prioritize recall. This is because customer retention is much more important than preventing the incorrect classification of safe customers as churning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F-1 Score combines recall and precision. It has a more complex formula. It provides you with a balanced measure of a models performance. A model with a high F1 score shows that it can identify positive cases while minimizing flase predictions. A low F1 score indicates a poor balance of Recall and Precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy measures the proportion of correct predictions out of the total predictions made by the model. It answers the quesiont, \"How often is the model correct?\"\n",
    "\n",
    "Accuracy doesn't always tell the full story, in fact accuracy alone is a poor assessment of our model. In our case, non-churned customers dominate the dataset. Accuracy may be high even though the model may fail to predict the minority class accurately at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the predictions, and determine the models accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, Y_test, verbose = 0)\n",
    "print(F\"Test Accuracy : {test_accuracy: .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy score is good. An accuracy of 0.5 represents complete randomness, and our model is significantly better than 0.5. We can effectively predict whether or not a specific customer will churn using the model we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast predictions to integers\n",
    "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Create confusion matrix\n",
    "con_matrix = confusion_matrix(Y_test, y_pred)\n",
    "\n",
    "# Display Confustion Matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sb.heatmap(con_matrix, annot=True, fmt='d', cmap='Blues', cbar = False)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Optimized Model Confusion Matrix\")\n",
    "\n",
    "if os.path.exists(\"/kaggle/\"):\n",
    "    save_path = \"/kaggle/working/\"\n",
    "else:\n",
    "    save_path = os.path.join(os.getcwd(), \"plots\")\n",
    "\n",
    "os.makedirs(save_path, exist_ok = True)\n",
    "\n",
    "plt.savefig(os.path.join(save_path, \"good_confusion_matrix.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "def analyze_model(test_data, pred_data, name):\n",
    "    print(f\"{name} recall    = {recall_score(test_data, pred_data):.2f}\")\n",
    "    print(f\"{name} precision = {precision_score(test_data, pred_data):.2f}\")\n",
    "    print(f\"\\n\")\n",
    "    print(f\"{name} f1-Score  = {f1_score(test_data, pred_data):.2f}\")\n",
    "    print(f\"{name} accuracy  = {accuracy_score(test_data, pred_data):.2f}\")\n",
    "\n",
    "analyze_model(Y_test, y_pred, \"Optimized model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 1300 customers were retained, and the model classified them as such. 225 customers were mistakenly predicted to churn. Only about  customers left the bank without the model predicting it. 275 customers were predicted to churn, and the prediction was accurate. \n",
    "\n",
    "I made a very important decision when training the model, and it was to weight the classes. Using \"class_weights = {0: 1.0, 1: 3.5}\", means that the neural network will penalize itself 3.5 times harder when it misclassifies a churned customer.\n",
    "\n",
    "I could improve the \"accuracy\" of the model by getting rid of this weighting, however that would lead to highly accurate retained customer prediction, and more innacurate churned customer prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Optimal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the cell below. Notice the class weights at the top of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes are evenly weighted (default behavior)\n",
    "class_weights = {0: 1.0, 1: 1.0}\n",
    "\n",
    "\n",
    "# recompile cloned model\n",
    "tune2 = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model2.compile(optimizer=tune2,\n",
    "               loss = 'binary_crossentropy',\n",
    "               metrics = ['accuracy'])\n",
    "\n",
    "# Train model 2\n",
    "history2 = model2.fit(X_train_scaled, Y_train,    \n",
    "                    class_weight=class_weights, \n",
    "                    epochs = 75,                \n",
    "                    batch_size = 32,            \n",
    "                    validation_split = 0.2,     \n",
    "                    callbacks = [early_stop],   \n",
    "                    verbose = 0)     \n",
    "\n",
    "# Cast predictions to integers\n",
    "y_pred2 = (model2.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Create confusion matrix\n",
    "con_matrix2 = confusion_matrix(Y_test, y_pred2)\n",
    "\n",
    "# Display Confustion Matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sb.heatmap(con_matrix2, annot=True, fmt='d', cmap='Blues', cbar = False)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Poorly Optimized Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "analyze_model(Y_test, y_pred2, \"Poorly optimized model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this un-weighted model, it is very good at predicting retained customers, but practically useless at providing insights on which customers will actually leave the bank.\n",
    "\n",
    "This is the most \"precise\" and \"accurate\" model, but it is the poorest performing model. This is because the recall and the f1-score are the most important metrics for analyzing this dataset.\n",
    "\n",
    "The recall on this model is about 0.5, which means that the model is practically guessing whether or not a churned customer will be retained or not. Each churned customer is worth a lot of money to the bank, so we want to minimize the recall as much as possible. Let's look at out original confusion matrix again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(os.path.join(save_path, \"good_confusion_matrix.png\"), width = 600))\n",
    "analyze_model(Y_test, y_pred, \"Optimized model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this model is slightly less accurate, is has a significantly improved recall. We trade off 0.10 precision, 0.29 f1-score, and 0.05 accuracy for a 35% increase in our recall rate. \n",
    "\n",
    "This means that our optimized model is better able to predict which customers will churn, at a comparitively small cost of every other statistic. Since the churned customers are critical, this would be the more effective model to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall Optimized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The recall rate could be further improved by increasing the class weight for churned customers.\n",
    "\n",
    "Increasing the recall comes at the cost of increasing the false positive ocurrance rate.\n",
    "\n",
    "This is the line where the threshold is decreased :\n",
    "\n",
    "`y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")`\n",
    "\n",
    "In this model, I will increase the weight for churned customers to 4.0, and decrease the prediction threshold for churned customers to 0.4, down from 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churned customers are heavily weighted\n",
    "class_weights3 = {0: 1.0, 1: 4.0}\n",
    "\n",
    "# Recompile cloned model\n",
    "tune3 = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model3.compile(optimizer=tune3,\n",
    "               loss = 'binary_crossentropy',\n",
    "               metrics = ['accuracy'])\n",
    "\n",
    "# Train model 3\n",
    "history3 = model3.fit(X_train_scaled, Y_train,    \n",
    "                    class_weight=class_weights, \n",
    "                    epochs = 75,                \n",
    "                    batch_size = 32,            \n",
    "                    validation_split = 0.2,     \n",
    "                    callbacks = [early_stop],   \n",
    "                    verbose = 0)     \n",
    "\n",
    "\n",
    "# Super recall optimized model\n",
    "y_pred3 = (model.predict(X_test_scaled) > 0.4).astype(\"int32\")# 0.5 -> 0.4\n",
    "con_matrix3 = confusion_matrix(Y_test, y_pred3)\n",
    "\n",
    "# Display Confustion Matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sb.heatmap(con_matrix3, annot=True, fmt='d', cmap='Blues', cbar = False)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Recall Optimized Matrix\")\n",
    "plt.show()\n",
    "\n",
    "analyze_model(Y_test, y_pred3, \"Recall optimized model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this threshold, the recall is significantly improved, and the false positive rate is also significantly increased. \n",
    "\n",
    "Notice how the overall accuracy decreases. This would be an acceptable trade off if we needed to minimize false negatives as much as possible. In our case, this could be overkill. Around 700/2000 customers are predicted to churn, and about 80% of the churned customers are predicted by the model.\n",
    "\n",
    "If we were working with medical data, this would be the bare minimum. If we were working with strictly high profile (high balance) bank customers, this model could be a good idea.\n",
    "\n",
    "However, since we are working with a mix of low and high balance customers, I think the balanced model will be the best pick overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(os.path.join(save_path, \"good_confusion_matrix.png\"), width = 600))\n",
    "analyze_model(Y_test, y_pred, \"Optimized model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further model development could potentially involve dynamically decreasing the threshold for customers with a very high balance. This would be a good idea since customers with a high balance are very valuable to the bank, and this approach would decrease the number of False Positive Occurrances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "to-do: \n",
    "\n",
    "       - Write Introduction\n",
    "       - Write Conclusion\n",
    "       - Review + Document\n",
    "       - Publish\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
